# Uso de Hiperparámetros

## Objetivo

En este tutorial aprenderás a:

- 


## Configuración inicial

Importamos las librerías necesarias, cargamos la API key desde un .env y creamos el cliente de OpenAI.


```{python}
from openai import OpenAI
import os
from IPython.display import display, Markdown
```
```{python}
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

client = OpenAI(api_key=api_key)
```

## Temperature (float: 0-2)

Controla la aleatoriedad de las respuestas. Valores más altos producen respuestas más creativas y diversas.

- 0.0: Respuestas consistentes y determinísticas
- 0.5: Balance entre creatividad y consistencia
- 1.0: Mayor creatividad y variabilidad
- 2.0 Máxima aleatoriedad


```{python}
prompt = 'las nubes estan algo grises verdad?'
# Temperatura baja (0.2)
response_conservador = client.chat.completions.create(
    model='gpt-4o-mini',
    messages=[{
        'role': 'user',
        'content': prompt
    }],
    temperature=0.2
)

```
Miramos la respuesta conservadora:

```{python}
display(Markdown(response_conservador.choices[0].message.content))
```

```{python}

# Temperatura alta (1.8)
response_creativo = client.chat.completions.create(
    model='gpt-4o-mini',
    messages=[{
        'role': 'user',
        'content': prompt
    }],
    temperature=1.5
)
```


Miremos ahora la respuesta creativa:

```{python}
display(Markdown(response_creativo.choices[0].message.content))
```

Probemos ahora con otro ejemplo

```{python}
prompt = 'Genera un eslogan para una cafeteria en la luna'
# Temperatura baja (0.2)
response_conservador = client.chat.completions.create(
    model='gpt-4o-mini',
    messages=[{
        'role': 'user',
        'content': prompt
    }],
    temperature=0.2
)

# Temperatura alta (1.8)
response_creativo = client.chat.completions.create(
    model='gpt-4o-mini',
    messages=[{
        'role': 'user',
        'content': prompt
    }],
    temperature=1.5
)
```

Miramos la respuesta conservadora:

```{python}
display(Markdown(response_conservador.choices[0].message.content))
```

Miremos ahora la respuesta creativa:

```{python}
display(Markdown(response_creativo.choices[0].message.content))
```

## max_tokens

Define la longitud máxima de la respuesta en tokens. Un token equivale a 4 caracteres o 3/4 de una palabra promedio en ingles.

Consideraciones:

- Muy bajo (<50): Las respuestas pueden quedar incompletas
- Medio (100-500): Ideal para respuestas generales
- Alto (>1000): Para generación de contenido extenso

```{python}
# Respuesta corta
response_corta = client.chat.completions.create(
    model='gpt-4o-mini',
    messages=[{'role': 'user', 'content': 'Describe un atardecer'}],
    max_tokens=50
)

# Respuesta larga
response_larga = client.chat.completions.create(
    model='gpt-4o-mini',
    messages=[{'role': 'user', 'content': 'Describe un atardecer'}],
    max_tokens=200
)
```

Imprimimos los mensajes

```{python}
print(response_corta.choices[0].message.content)
print(100*'-')
print(response_larga.choices[0].message.content)
```

## top_p (float: 0-1)

Nucleus sampling: controla la diversidad seleccionando tokens cuya probabilidad suma 'top_p'.

- 0.1: Muy conservador, usa solo las palabras más probables.
- 0.5: Balance entre diversidad y precisión.
- 0.9: Mayor variabilidad en el vocabulario.

Nota: Generalmente se usa o 'temperature' o 'top_p', no ambos. Ejemplo de uso: top_p=0.9 Permite respuestas variadas pero coherentes.


```{python}
for top_p in [0.1, 0.5, 0.9]:
    response = client.chat.completions.create(

    )
```